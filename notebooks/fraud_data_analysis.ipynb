{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c3c12a",
   "metadata": {},
   "source": [
    "# Fraud Detection - Data Analysis and Preprocessing\n",
    "\n",
    "This notebook focuses on Phase 2 (Data Understanding & Exploration) and Phase 3 (Data Preprocessing & Feature Engineering) of our fraud detection project.\n",
    "\n",
    "## Phase 2: Data Understanding & Exploration\n",
    "- Load and explore dataset\n",
    "- Analyze class distribution\n",
    "- Check for missing values\n",
    "- Visualize key patterns\n",
    "- Document findings\n",
    "\n",
    "## Phase 3: Data Preprocessing & Feature Engineering\n",
    "- Clean data and handle missing values\n",
    "- Feature engineering\n",
    "- Address class imbalance\n",
    "- Prepare train/test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119609a5",
   "metadata": {},
   "source": [
    "## Setup and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abff19",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset\n",
    "\n",
    "Let's load our fraud detection dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_fraud_dataset(filepath='../data/transactions.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"Dataset not found. Using synthetic data for demonstration.\")\n",
    "        # Create synthetic data\n",
    "        n_samples = 10000\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate synthetic transaction data\n",
    "        data = {\n",
    "            'transaction_id': range(n_samples),\n",
    "            'timestamp': pd.date_range(start='2025-01-01', periods=n_samples, freq='H'),\n",
    "            'amount': np.random.exponential(100, n_samples),\n",
    "            'merchant_id': np.random.randint(1, 1000, n_samples),\n",
    "            'customer_id': np.random.randint(1, 5000, n_samples),\n",
    "            'merchant_category': np.random.choice(['retail', 'online', 'travel', 'entertainment'], n_samples),\n",
    "            'fraud': np.random.choice([0, 1], n_samples, p=[0.99, 0.01])  # 1% fraud rate\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"Synthetic dataset created with shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "# Load the data\n",
    "df = load_fraud_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9512f08",
   "metadata": {},
   "source": [
    "### Basic Data Analysis\n",
    "\n",
    "Let's analyze the basic characteristics of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfe1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(\"-\" * 50)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(df.describe())\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nFraud Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "fraud_dist = df['fraud'].value_counts(normalize=True)\n",
    "print(fraud_dist)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = fraud_dist[0] / fraud_dist[1]\n",
    "print(f\"\\nImbalance ratio (non-fraud:fraud): {imbalance_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703682d",
   "metadata": {},
   "source": [
    "### Visualization of Key Patterns\n",
    "\n",
    "Let's visualize some key patterns in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting area\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Amount Distribution by Fraud Status\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='fraud', y='amount', data=df)\n",
    "plt.title('Transaction Amount Distribution by Fraud Status')\n",
    "plt.xlabel('Fraud (0=No, 1=Yes)')\n",
    "plt.ylabel('Amount')\n",
    "\n",
    "# 2. Fraud Rate by Merchant Category\n",
    "plt.subplot(2, 2, 2)\n",
    "fraud_by_category = df.groupby('merchant_category')['fraud'].mean()\n",
    "fraud_by_category.plot(kind='bar')\n",
    "plt.title('Fraud Rate by Merchant Category')\n",
    "plt.xlabel('Merchant Category')\n",
    "plt.ylabel('Fraud Rate')\n",
    "\n",
    "# 3. Transaction Volume by Hour\n",
    "plt.subplot(2, 2, 3)\n",
    "df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "hourly_volume = df.groupby('hour')['transaction_id'].count()\n",
    "hourly_volume.plot(kind='line')\n",
    "plt.title('Transaction Volume by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# 4. Amount Distribution (Log Scale)\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(data=df, x='amount', hue='fraud', bins=50, log_scale=True)\n",
    "plt.title('Amount Distribution by Fraud Status (Log Scale)')\n",
    "plt.xlabel('Amount (Log Scale)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some key findings\n",
    "print(\"Key Findings:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"1. Average transaction amount: ${df['amount'].mean():.2f}\")\n",
    "print(f\"2. Fraud transaction average: ${df[df['fraud']==1]['amount'].mean():.2f}\")\n",
    "print(f\"3. Non-fraud transaction average: ${df[df['fraud']==0]['amount'].mean():.2f}\")\n",
    "print(f\"4. Most common merchant category: {df['merchant_category'].mode()[0]}\")\n",
    "print(f\"5. Peak transaction hour: {hourly_volume.idxmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffea74",
   "metadata": {},
   "source": [
    "## Phase 3: Data Preprocessing & Feature Engineering\n",
    "\n",
    "Now let's prepare our data for modeling by:\n",
    "1. Handling missing values\n",
    "2. Feature engineering\n",
    "3. Encoding categorical variables\n",
    "4. Addressing class imbalance\n",
    "5. Creating train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Function\n",
    "def engineer_features(df):\n",
    "    # Create copy to avoid modifying original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 1. Time-based features\n",
    "    df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'])\n",
    "    df_processed['hour'] = df_processed['timestamp'].dt.hour\n",
    "    df_processed['day_of_week'] = df_processed['timestamp'].dt.dayofweek\n",
    "    df_processed['is_weekend'] = df_processed['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # 2. Amount-based features\n",
    "    df_processed['amount_log'] = np.log1p(df_processed['amount'])\n",
    "    \n",
    "    # 3. Transaction frequency features\n",
    "    customer_tx_counts = df_processed.groupby('customer_id')['transaction_id'].count()\n",
    "    merchant_tx_counts = df_processed.groupby('merchant_id')['transaction_id'].count()\n",
    "    \n",
    "    df_processed['customer_tx_count'] = df_processed['customer_id'].map(customer_tx_counts)\n",
    "    df_processed['merchant_tx_count'] = df_processed['merchant_id'].map(merchant_tx_counts)\n",
    "    \n",
    "    # 4. Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    df_processed['merchant_category_encoded'] = le.fit_transform(df_processed['merchant_category'])\n",
    "    \n",
    "    # 5. Drop original columns we don't need\n",
    "    columns_to_drop = ['transaction_id', 'timestamp', 'merchant_category']\n",
    "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Process the data\n",
    "df_processed = engineer_features(df)\n",
    "print(\"Processed features:\")\n",
    "print(df_processed.columns.tolist())\n",
    "\n",
    "# Split features and target\n",
    "X = df_processed.drop('fraud', axis=1)\n",
    "y = df_processed['fraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Training set (after SMOTE): {X_train_scaled.shape}\")\n",
    "print(f\"Testing set: {X_test_scaled.shape}\")\n",
    "print(f\"\\nClass distribution in training set (after SMOTE):\")\n",
    "print(pd.Series(y_train_resampled).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc52f9",
   "metadata": {},
   "source": [
    "### Save Processed Data\n",
    "\n",
    "Let's save our processed datasets for later use in modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071cc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed data directory if it doesn't exist\n",
    "import os\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save processed datasets\n",
    "np.save(f'{processed_dir}/X_train_scaled.npy', X_train_scaled)\n",
    "np.save(f'{processed_dir}/X_test_scaled.npy', X_test_scaled)\n",
    "np.save(f'{processed_dir}/y_train_resampled.npy', y_train_resampled)\n",
    "np.save(f'{processed_dir}/y_test.npy', y_test)\n",
    "\n",
    "# Save feature names for reference\n",
    "pd.Series(X.columns).to_csv(f'{processed_dir}/feature_names.csv', index=False)\n",
    "\n",
    "# Save scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, f'{processed_dir}/scaler.joblib')\n",
    "\n",
    "print(\"Processed data saved successfully!\")\n",
    "print(f\"Files saved in: {processed_dir}\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(os.listdir(processed_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee64162",
   "metadata": {},
   "source": [
    "## Summary of Data Analysis and Preprocessing\n",
    "\n",
    "Key findings and steps completed:\n",
    "\n",
    "1. Data Exploration:\n",
    "   - Dataset shape and basic statistics analyzed\n",
    "   - Class imbalance identified and quantified\n",
    "   - Visualized key patterns in transaction amounts and timing\n",
    "   - Analyzed fraud patterns across merchant categories\n",
    "\n",
    "2. Feature Engineering:\n",
    "   - Created time-based features (hour, day of week, weekend flag)\n",
    "   - Added transaction frequency features\n",
    "   - Transformed amount using log scaling\n",
    "   - Encoded categorical variables\n",
    "\n",
    "3. Preprocessing:\n",
    "   - Handled class imbalance using SMOTE\n",
    "   - Scaled features using StandardScaler\n",
    "   - Split data into training and test sets\n",
    "   - Saved processed datasets for modeling\n",
    "\n",
    "Next Steps:\n",
    "1. Develop quantum computing model using processed features\n",
    "2. Implement model evaluation metrics\n",
    "3. Create integration interface for Java application"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
